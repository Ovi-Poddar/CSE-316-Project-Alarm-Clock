{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": [
        "6O0tTO4fj91q",
        "ghbPgDGt1Ojz",
        "hjnnoRvarEyD",
        "YHQ6jSw-q2mX"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ovi-Poddar/CSE-316-Project-Alarm-Clock/blob/main/Image_Caption_Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pushing the notebook to the repository\n",
        "---\n",
        "\n",
        "After making changes to your notebook, please commit and push them to the repository.\n",
        "1. To do so from within a Colab notebook, click `File â†’ Save a copy in GitHub`\n",
        "2. You will be prompted to add a commit message, and after you click OK, the notebook will be pushed to your repository\n",
        "3. Please push it to [GitHub - rng70/Image-Caption-Generation: image caption generation using rnn and cnn](https://github.com/rng70/Image-Caption-Generation)\n"
      ],
      "metadata": {
        "id": "dDxiipvB5smA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "Teb5woo8HsVd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mount Google Drive\n",
        "---\n",
        "\n",
        "We need dataset before any kind of analysis. Here, data from google drive is mounted for making dataset\n",
        "\n",
        "* Folder Structure: <br>\n",
        "    &nbsp;&nbsp;&nbsp;&nbsp;/My Drive/flickr8k-dataset/{dataset-directory-name}\n",
        "    <br>\n",
        "    <br>\n",
        "* Available Dataset: <br>\n",
        "    01. &nbsp;&nbsp;&nbsp;&nbsp;Flickr8k_Dataset/\n",
        "    02. &nbsp;&nbsp;&nbsp;&nbsp;Flickr8k_text/\n"
      ],
      "metadata": {
        "id": "6O0tTO4fj91q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "id": "0O8sLPPWqB1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "oe50ryZJzKX6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Preperation"
      ],
      "metadata": {
        "id": "ghbPgDGt1Ojz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check python version\n",
        "!python --version"
      ],
      "metadata": {
        "id": "UvOPV3iP2BUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader,Dataset\n",
        "import torchvision.transforms as T"
      ],
      "metadata": {
        "id": "g_ETxam62F4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def getWindows(input, output_size, kernel_size, padding=0, stride=1, dilate=0):\n",
        "    working_input = input\n",
        "    working_pad = padding\n",
        "    # dilate the input if necessary\n",
        "    if dilate != 0:\n",
        "        working_input = np.insert(working_input, range(1, input.shape[2]), 0, axis=2)\n",
        "        working_input = np.insert(working_input, range(1, input.shape[3]), 0, axis=3)\n",
        "\n",
        "    # pad the input if necessary\n",
        "    if working_pad != 0:\n",
        "        working_input = np.pad(working_input, pad_width=((0,), (0,), (working_pad,), (working_pad,)), mode='constant', constant_values=(0.,))\n",
        "\n",
        "    in_b, in_c, out_h, out_w = output_size\n",
        "    out_b, out_c, _, _ = input.shape\n",
        "    batch_str, channel_str, kern_h_str, kern_w_str = working_input.strides\n",
        "    \n",
        "\n",
        "    return np.lib.stride_tricks.as_strided(\n",
        "        working_input,\n",
        "        (out_b, out_c, out_h, out_w, kernel_size, kernel_size),\n",
        "        (batch_str, channel_str, stride * kern_h_str, stride * kern_w_str, kern_h_str, kern_w_str)\n",
        "    )\n",
        "\n",
        "\n",
        "class Conv2D:\n",
        "    \"\"\"\n",
        "    An implementation of the convolutional layer. We convolve the input with out_channels different filters\n",
        "    and each filter spans all channels in the input.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=0):\n",
        "        \"\"\"\n",
        "        :param in_channels: the number of channels of the input data\n",
        "        :param out_channels: the number of channels of the output(aka the number of filters applied in the layer)\n",
        "        :param kernel_size: the specified size of the kernel(both height and width)\n",
        "        :param stride: the stride of convolution\n",
        "        :param padding: the size of padding. Pad zeros to the input with padding size.\n",
        "        \"\"\"\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "\n",
        "        self.cache = None\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        self.weight = 1e-3 * np.random.randn(self.out_channels, self.in_channels,  self.kernel_size, self.kernel_size)\n",
        "        self.bias = np.zeros(self.out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        The forward pass of convolution\n",
        "        :param x: input data of shape (N, C, H, W)\n",
        "        :return: output data of shape (N, self.out_channels, H', W') where H' and W' are determined by the convolution\n",
        "                 parameters.\n",
        "        \"\"\"\n",
        "\n",
        "        n, c, h, w = x.shape\n",
        "        out_h = (h - self.kernel_size + 2 * self.padding) // self.stride + 1\n",
        "        out_w = (w - self.kernel_size + 2 * self.padding) // self.stride + 1\n",
        "\n",
        "        windows = getWindows(x, (n, c, out_h, out_w), self.kernel_size, self.padding, self.stride)\n",
        "        for window in windows:\n",
        "          print(\"==============================================================\")\n",
        "          print(\"Fucking Windowds\")\n",
        "          print(window)\n",
        "          print(\"==============================================================\")\n",
        "\n",
        "        out = np.einsum('bihwkl,oikl->bohw', windows, self.weight)\n",
        "\n",
        "        # add bias to kernels\n",
        "        out += self.bias[None, :, None, None]\n",
        "\n",
        "        self.cache = x, windows\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        \"\"\"\n",
        "        The backward pass of convolution\n",
        "        :param dout: upstream gradients\n",
        "        :return: dx, dw, and db relative to this module\n",
        "        \"\"\"\n",
        "        x, windows = self.cache\n",
        "\n",
        "        padding = self.kernel_size - 1 if self.padding == 0 else self.padding\n",
        "\n",
        "        dout_windows = getWindows(dout, x.shape, self.kernel_size, padding=padding, stride=1, dilate=self.stride - 1)\n",
        "        rot_kern = np.rot90(self.weight, 2, axes=(2, 3))\n",
        "\n",
        "        db = np.sum(dout, axis=(0, 2, 3))\n",
        "        dw = np.einsum('bihwkl,bohw->oikl', windows, dout)\n",
        "        dx = np.einsum('bohwkl,oikl->bihw', dout_windows, rot_kern)\n",
        "\n",
        "        return db, dw, "
      ],
      "metadata": {
        "id": "fLWwcdXNFkT_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "in_channels = 1\n",
        "out_channels = 128\n",
        "kernel_size = 3\n",
        "stride = 2\n",
        "padding = 1\n",
        "batch_size = (4, in_channels, 12, 10)  # expected input size\n",
        "dout_size = (4, out_channels, 6, 5)   # expected to match the size of the layer outputs\n",
        "\n",
        "np.random.seed(42)  # for reproducibility"
      ],
      "metadata": {
        "id": "teIg_D_BU1Dc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.random.random(batch_size)  # create data for forward pass\n",
        "dout = np.random.random(dout_size)  # create random data for backward\n",
        "#print(x)\n",
        "print('x: ', x.shape)\n",
        "#print('d_out: ', dout.shape)"
      ],
      "metadata": {
        "id": "B1GTmfWgU6Np"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conv = Conv2D(in_channels, out_channels, kernel_size, stride, padding)"
      ],
      "metadata": {
        "id": "9dY2ryYiU-Qx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conv_out = conv.forward(x)\n",
        "# db, dw, dx = conv.backward(dout)\n",
        "\n",
        "print('conv_out: ', conv_out.shape)\n"
      ],
      "metadata": {
        "id": "0rXSTb_wVCUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set Up"
      ],
      "metadata": {
        "id": "hjnnoRvarEyD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U tensorflow_text tensorflow tensorflow_datasets"
      ],
      "metadata": {
        "id": "nd9Fj24PrIGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install einops"
      ],
      "metadata": {
        "id": "dFLjUZP-rQGK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries"
      ],
      "metadata": {
        "id": "YHQ6jSw-q2mX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import concurrent.futures\n",
        "import collections\n",
        "import dataclasses\n",
        "import hashlib\n",
        "import itertools\n",
        "import json\n",
        "import math\n",
        "import os\n",
        "import pathlib\n",
        "import random\n",
        "import re\n",
        "import string\n",
        "import time\n",
        "import urllib.request\n",
        "\n",
        "import einops\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import requests\n",
        "import tqdm\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "import tensorflow_datasets as tfds"
      ],
      "metadata": {
        "id": "yIieAAj3ryGM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Data"
      ],
      "metadata": {
        "id": "p1LI4NrozRcg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path to the Flickr8k dataset in your Google Drive\n",
        "flickr8k_dir = '/content/drive/MyDrive/flickr8k-dataset/flickr8k'"
      ],
      "metadata": {
        "id": "9okC435ZrQ7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Images"
      ],
      "metadata": {
        "id": "NppvcPKV_kOA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "image_dir = os.path.join(flickr8k_dir, 'Flickr8k_Images')\n",
        "image_files = glob.glob(image_dir + \"/*\") # Get a list of all image file paths"
      ],
      "metadata": {
        "id": "-E6t_wjv_uCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(image_files))"
      ],
      "metadata": {
        "id": "_9CX3G1EAbMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Captions"
      ],
      "metadata": {
        "id": "TjxJtcMi_ZN7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the captions for the Flickr8k dataset\n",
        "captions_filename = os.path.join(flickr8k_dir, 'Flickr8k_Captions', 'captions.txt')\n",
        "captions_file = open(captions_filename, 'r')\n",
        "captions = captions_file.readlines()[1:]\n",
        "captions_file.close()"
      ],
      "metadata": {
        "id": "Y32rFenJqUnL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataFrame to store the captions\n",
        "captions_df = pd.DataFrame(columns=['image_id', 'caption'])\n",
        "\n",
        "# Extract the image ID and caption from each line of the captions file and add them to the DataFrame\n",
        "for caption in captions:\n",
        "    if(len(captions_df) > 100) : \n",
        "      break\n",
        "    image_id, caption_text = caption.strip().split(',', 1)\n",
        "    captions_df = captions_df.append({'image_id': image_id, 'caption': caption_text}, ignore_index=True)\n",
        "\n",
        "captions_df.head(5)"
      ],
      "metadata": {
        "id": "94VZD2XExIvw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zK_S58P07luI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Image feature extractor\n",
        "We will use an image model (pretrained on imagenet) to extract the features from each image. The model was trained as an image classifier, but setting `include_top=False` returns the model without the final classification layer, so we can use the last layer of feature-maps:"
      ],
      "metadata": {
        "id": "6d3BTF2G5OpR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_SHAPE=(224, 224, 3)\n",
        "mobilenet = tf.keras.applications.MobileNetV3Small(\n",
        "    input_shape=IMAGE_SHAPE,\n",
        "    include_top=False,\n",
        "    include_preprocessing=True)\n",
        "mobilenet.trainable=False"
      ],
      "metadata": {
        "id": "JBjntFDQ5SkZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to load an image and resize it for the model:\n",
        "def load_image(image_path):\n",
        "    img = tf.io.read_file(image_path)\n",
        "    img = tf.io.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.resize(img, IMAGE_SHAPE[:-1])\n",
        "    return img"
      ],
      "metadata": {
        "id": "5pLOxq4Z5ppZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`load_image(ex_path)` returns a single image tensor with shape **(height, width, channels)**, where height, width, and channels depend on the image. The `tf.newaxis` expression creates a new dimension of size 1, which turns the image tensor into a batch of size 1 with shape **(1, height, width, channels)**. The `:` expression then selects all elements along the remaining dimensions, effectively creating a batch with a single element that corresponds to the image."
      ],
      "metadata": {
        "id": "KjFKMaYGMtz_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The model returns a feature map for each image in the input batch:\n",
        "ex_path = image_files[0]\n",
        "test_img_batch = load_image(ex_path)[tf.newaxis, :]\n",
        "\n",
        "print(test_img_batch.shape)\n",
        "print(mobilenet(test_img_batch).shape)"
      ],
      "metadata": {
        "id": "VKk2SV9l7JQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup the text tokenizer/vectorizer"
      ],
      "metadata": {
        "id": "jWjsn7_8Naff"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1sQzSuu3Nbnu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}